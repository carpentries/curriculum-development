# Assembling the lesson

* How to write challenges (you)
* Different types of challenges and why / how to use them
* How to test challenges for correct level
* How to space challenges
* Writing the live code (we)
* What to write for instructor notes
* Writing the narrative around the code (I)
* How to evaluate lesson drafts for expert blind spot, correct leveling and scope
* Issue Bonanza (limited - CAC?)
* Clean-up (authors and staff)
* Put workshop page together
* We now have the alpha version of the lesson. (?)

---

## Picking a dataset

The dataset is a critical element of a Carpentries lesson. It needs to be chosen carefully
and to meet the following criteria.

1. **Use a single dataset** -- Curricula are domain-specific and the same dataset should be
used across all lessons that are part of the same curriculum. When developing a
standalone lesson (one that is not part of a curriculum), we encourage you to choose a
dataset that is already in
use in one of our existing curricula. Although each lesson should use the same dataset, 
it is often appropriate to use variations of the core dataset for different lessons within a
curriculum. For instance, the Data Carpentry lessons on data organization with spreadsheets
use messy spreadsheets that have been created from the
original dataset, but which introduce formatting issues to teach tidy data principles. 
Whenever possible, these derived datasets should be created using scripts rather than
manually, so they can be regenerated if the original dataset changes.

1. **The dataset should be released under a CC0 license** -- Copyright laws and laws
governing use and sharing of data and databases vary among countries. The [Creative Commons
Zero](https://creativecommons.org/share-your-work/public-domain/cc0/) (CC0) license is designed to allow unrestricted use and sharing of data universally. The CC0
license allows the development of lessons around the dataset and modification of the dataset
to suit our teaching needs.

1. **The dataset should be deposited in a public repository** -- All variations of the 
dataset that are used in the lesson should be deposited. The Carpentries deposits
data for our lessons on [figshare](https://figshare.com/). 
If you choose another option, make sure the repository where the data is archived
has the following features: 
   - a DOI link pointing to an overview of the dataset
   - pre-registration of the DOI
   - all files can be downloaded directly as an archive (e.g., zip file) with a persistent link
   - each file can be downloaded directly with a persistent link
   - the repository supports versioning

1. **The dataset should be real and represent what researchers in the field encounter** -- 
The datasets used as examples in the
lessons should be based on real research datasets, and be of sufficient complexity that they
are representative of the type of dataset that learners would encounter in their own
research.

1. **Authors of the dataset should be identifiable, acknowledged, and there should be a link to the original source for the data** -- Even though the datasets we use in our lessons are
released under a CC0 license, we acknowledge the authors of the dataset and link to
the research projects based on the data we use.

1. **The dataset should be large enough** -- Analysing the dataset should represent a real
challenge that
highlights the power and usefulness of the tools covered in the lessons. The dataset should
be larger than what would be easy to analyze and manipulate in a spreadsheet program. It 
should be similar in size to what researchers in that domain work with in their actual
research. For instance, the core dataset for the [Data Carpentry Ecology curriculum](https://datacarpentry.org/ecology-workshop/data/) has ~35,000 rows.

1. **The dataset should be complex enough to ask interesting questions** -- Each observation
should have at least 4-5 variables. These variables should be of a few
different data types (at least continuous, discrete, integers, real numbers; and depending
on the domain, may include more specialized data types such as date/time, GPS coordinates, unstructured text, etc.)

1. **The motivation for study and the protocol for data collection should be understable without much context** -- We have limited time in our workshops to cover the technical
skills we want to teach. It should not take long to explain to learners what the data is about,
how it was collected, and what types of interesting questions can be asked from it.

1. **The dataset should be relevant in different geographical and cultural contexts** -- Our
workshops are taught to learners from diverse cultural and geographical backgrounds. The
dataset should be
understandable without much cultural context or pre-requiste knowledge needed
to make it compelling.

1. **There should be clear and comprehensive metadata** -- The metadata should include a
description of the data, explain what is included in each data field, how it was measured,
and the unit in which it is reported.

Overall, datasets used in Carpentries workshops should serve as examples of publicly
deposited
data suitable for research re-use. Learners should be able to use these datasets as examples
and guides for their own research data that they would like to publish and make available to
the broad scientific and academic community.

## Formatting the dataset for teaching

A possible challenge when using research datasets for teaching is that the dataset can
include complexity that makes teaching more difficult by unnecessarily increasing
learners' cognitive load. While it is important for the dataset to
provide an authentic experience for learners, it is often useful to simplify it or
do some initial data cleaning and wrangling to make it easier for learners
to focus on the core skills you are teaching. For instance, you
may want to edit the dataset so that missing values are parsed appropriately. 
You may also want to remove data which leads to errors or warnings during parsing, 
columns with data types
that are not relevant for the learning objectives of the workshops, or variables which 
require additional context to understand.

When preparing a dataset for teaching, aim to find a balance between providing an
authentic experience for learners while keeping complexity low to limit distractions from the
learning objectives. Depending on the lesson's goals, it might also be interesting to
include several versions of the datasets that have undergone various levels of processing.
At the beginning of the lesson, you can provide a clean and well organized dataset, while
later you can introduce more complexity and teach learners how to handle it to generate the cleaner
version of the data. Don't introduce too many (no more than three) versions of the dataset in
your lessons, as dealing with many files and remembering their differences can become
challenging for the learners.

## Overview 

Carpentries workshops last two full days. Generally workshops run from 9am to 5pm, with 30-minute breaks in the morning and in the afternoon, and an hour lunch break, leaving 6 hours for instruction. Carpentries lessons are therefore designed to be taught in 1.5, 3 or 6 hours. Having long and regular breaks are needed to pace the instruction, and allow learners to remain focused and engaged.

Lessons are divided into episodes. Each episode should:

- be designed to last 15-30 minutes;
- limit cognitive load by introducing at most 7($\pm$ 2) new concepts (3 to 5 concepts preferred);
- include at least one to three challenges that let learners practice the concepts covered in the episode.


## Designing challenges

Once the learning objectives for the lesson and each episode have been identified, you can start
designing challenges that provide learners an opportunity to practice the skills outlined in the
learning objectives. The Bloom taxonomy level of the learning objectives help frame how much
information should be provided to the learners to be able to complete the exercise and the type
of exercise. Writing the challenges before writing the content of the lesson ensures that the
lesson will remain focused, and can reveal concepts needed to solve the challenges that were not
previously considered.

Starting with two to four challenges you want learners to be able to complete on their own at the
end of the training, will help you identify the skills they need to acquire. Creating this list
of skills will guide you in assembling your learning objectives. This list, in combination with
the learners' profiles, and the time you have available in your training, will help you identify
the prerequisite skills for the audience of the workshop, and determine at which level of the
Bloom taxonomy the learning objectives need to be written. This approach is "meet your learners
where they are". [FIXME: this paragraph might belong better to the learning objective section].

When starting to design challenges, it is helpful to start by planning the last exercise of each
episode. Identifying what these final challenges look like, and the skills they require, will
also help you
to craft the narrative for your lesson. The final challenges should be designed to be a natural
culmination of all of the skills and concepts that learners are taught during the lesson. 
Starting from the end allows you to design the rest of the lesson's challenges to 
build the skills and confidence learners need to
be able to solve the final challenges. 

The challenges in a lesson should be a mixture of direct application challenges and synthesis
challenges. A direct application challenge is a straightforward implementation of a concept
that learners have just been exposed to, while a synthesis challenge requires learners to 
integrate recently learned skills with skills that were covered earlier in
the lesson. Learning is reinforced when Instructors explicitly point out how the skills seen in
earlier parts of the lesson are being integrated to solve the challenges.

Challenges in Carpentries lessons are a form of formative assessment. They help learners further
their learning by having a chance to put into practice the skills being taught. They also help
Instructors monitor the level of understanding in the classroom, and potentially catch
misconceptions in the learner's mental models that can be corrected in real time, before they
become ingrained.


## Different types of challenges

### Multiple Choice Questions

Multiple choice questions (MCQs) can be a useful tool for formative assessment if they are
designed such that each incorrect answer helps the Instructor to identify learners' 
misconceptions. Each incorrect answer should be a *plausible distractor* with *diagnostic power*. 
"Plausible" means that an answer looks like it could be right, and "diagnostic power" means
that each of the distractors helps the instructor figure out
what concepts learners are having difficulty with.  

For example, if learners are learning about subsetting data in `R` with the `dplyr` 
functions `filter()` and `select()`, you might ask them to determine which of the 
following code blocks will return only the values of the "Species" and "Petal.Width" variables
and only for observations of the "setosa" species where petal length
was greater than 1.5. 

a. 
```{r, echo=TRUE, eval=FALSE}
iris %>%
  filter(Species == "setosa" & Petal.Length > 1.5) %>%
  select(Species, Petal.Width)
```

This is the correct answer. 

b.
```{r, echo=TRUE, eval=FALSE}
iris %>%
  select(Species == "setosa" & Petal.Length > 1.5) %>%
  filter(Species, Petal.Width)
```

Learners who select this answer have a simple factual misconception. They've confused the 
`select()` and `filter()` functions. They need to be reminded that `filter()` is for 
subsetting by row and `select()` is for subsetting by column. 

c.
```{r, echo=TRUE, eval=FALSE}
iris %>%
  select(Species, Petal.Width)  %>%
  filter(Species == "setosa" & Petal.Length > 1.5)
```

Learners who select this answer have a conceptual misunderstanding that will may require more 
time and effort to correct. They haven't understood that the pipe (`%>%`) character only 
passes into the next command the output of the previous command, and **nothing else**. Since
they have used the `select()` function first, the "Petal.Length" column is no longer present
in the output and cannot be used for comparison. This misconception might be addressed by 
drawing a diagram and walking through what the data looks like at each step of the command. 
Another follow-up question using the same skills could then be used to assess whether learners
have understood the concept.

As illustrated above, formative assessments are most powerful when an instructor modifies their
instruction depending on the
results of the assessment. An instructor may learn they need to change their pace or review a
particular concept. 
Knowing how to respond to the results of a formative assessment is a skill that you will develop
over time. Making sure your assessments are designed to test only one or two concepts at a time
will help ensure that the feedback is useful. 

The process of developing diagnostic plausible distractors takes time and requires some
knowledge of what common
misconceptions are for a particular topic. This knowledge can come through teaching experience 
(yourself or others') and is sometimes formally defined through [concept inventories]().
One strength of The Carpentries community is that our lessons are taught over and over again
by different Instructors in different teaching contexts. Some of those Instructors give
feedback on challenges and misconceptions that their learners had. Our exercises are thus 
continuously improved by pooling the teaching experience of our 1,500+ strong Instructor
community!

### Parson's problems

One reason well-designed multiple choice questions are so useful is that they constrain
the problem space. Learners don't need to worry about all of the details of syntax and how
to spell all of the variable names, but can focus on just the concepts that the exercise
author intended them to focus on. Another type of formative assessment that provides this
benefit are Parson's problems. A Parson's problem is an exercise where learners are given a 
set of items (in our case, lines of code) and asked to put them into an appropriate order to
accomplish a specific task. The MCQ example given above could be formulated as a Parson's
problem:

```{r, echo=TRUE, eval=FALSE}
filter(Species == "setosa" & Petal.Length > 1.5) %>%
iris %>%
select(Species, Petal.Width)
```

A more difficult version of a Parson's problem might include lines that are not part of the 
solution (*distractors*):

```{r, echo=TRUE, eval=FALSE}
filter(Species == "setosa" & Petal.Length > 1.5) %>%
filter(Species, Petal.Width)
iris %>%
select(Species, Petal.Width)
select(Species == "setosa" & Petal.Length > 1.5) %>%
```

If this is the case, make sure learners know that not all of the code chunks need to be
included in their answer! 

Parson's problems are somewhat less structured that MCQs, which makes them slightly
better for preparing learners to tackle similar problems in their own work. However, this also
makes it more difficult for Instructors to diagnose learner misconceptions and adjust their 
teaching accordingly (because there are more possible responses). As will all of the challenge
types we will discuss in this chapter, MCQs and Parson's problems can be used in combination
to provide learners with both structure and appropriate levels of challenge. 

### Fill-in-the-blank problems

Fill-in-the blank problems can be thought of as the next level of decreasing structure after
MCQs and Parson's problems (although this depends on the number of lines in the Parson's
problem and the number of possible choices for filling in the blanks, among other factors). 
The following challenge (from the Software Carpentry lesson on 
[The Unix Shell](http://swcarpentry.github.io/shell-novice/)) illustrates one possible
application of fill-in-the-blank problems: 

> ## Moving to the Current Folder
>
> After running the following commands,
> Jamie realizes that she put the files `sucrose.dat` and `maltose.dat` into the wrong folder:
>
> ~~~
> $ ls -F
>  analyzed/ raw/
> $ ls -F analyzed
> fructose.dat glucose.dat maltose.dat sucrose.dat
> $ cd raw/
> ~~~
> {: .language-bash}
>
> Fill in the blanks to move these files to the current folder
> (i.e., the one she is currently in):
>
> ~~~
> $ mv ___/sucrose.dat  ___/maltose.dat ___
> ~~~
> {: .language-bash}
> > ## Solution
> > ```
> > $ mv ../analyzed/sucrose.dat ../analyzed/maltose.dat .
> > ```
> > {: .language-bash}
> > Recall that `..` refers to the parent directory (i.e. one above the current directory)
> > and that `.` refers to the current directory.
> {: .solution}
{: .challenge}

We can also apply this concept to our earlier example and ask learners to fill-in-the-blanks
to build a code block that will return only the values of the "Species" and "Petal.Width"
variables and only for observations of the "setosa" species where petal length
was greater than 1.5: 

```{r, echo=TRUE, eval=FALSE}
iris %>%
  filter(Species ___ "setosa" & ___ > 1.5) %>%
  select(Species, ___ )
```

The difficulty of a fill-in-the-blank problem can be adjusted by changing the number of blanks,
and by providing (or not providing) a "word bank" of options to use. You can even use a series
of similar fill-in-the-blank problems, increasing the number of blanks each time, to prepare 
learners to build a code chunk without scaffolding. When used in this way, fill-in-the-blank
problems are also called *faded examples*. We discuss the use of faded examples, and why they
are a useful tool, in more detail in 
[our Instructor Training course](https://carpentries.github.io/instructor-training/05-memory/index.html). 

### Switch the dataset


### Use the concept in a different context

Near vs far transfer
Take into account cognitive load, make sure you're not introducing new concepts in the exercise.
